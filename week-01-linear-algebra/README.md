# ğŸ“˜ Week 01 â€” Linear Algebra Foundations for Machine Learning

## ğŸ¯ Objective

Build **deep intuition** and **from-scratch implementations** of core linear algebra concepts that power Machine Learning algorithms.

This week focuses on:

- Understanding vectors and matrices **geometrically and algebraically**
- Implementing operations **without NumPy shortcuts**
- Connecting math directly to **real ML use cases**

---

## ğŸ§  Concepts Covered

### ğŸ”¹ Vectors

- Vector representation
- Vector addition & subtraction
- Scalar multiplication
- Vector magnitude (L2 norm)
- Geometric interpretation

### ğŸ”¹ Dot Product

- Algebraic definition
- Geometric intuition (similarity, projection)
- Why dot product appears everywhere in ML

### ğŸ”¹ Matrices

- Matrix as a linear transformation
- Matrixâ€“vector multiplication
- Matrixâ€“matrix multiplication
- Identity matrix (conceptual)
- Why data is represented as matrices in ML

---

## ğŸ› ï¸ Implementations (From Scratch)

All implementations are written **without using NumPy linear algebra utilities** to ensure true understanding.

### ğŸ“‚ `code/`

| File                       | Description                                       |
| -------------------------- | ------------------------------------------------- |
| `vector_operations.py`     | Vector addition, scalar multiplication, magnitude |
| `dot_product.py`           | Manual dot product implementation + tests         |
| `matrix_multiplication.py` | Matrixâ€“vector & matrixâ€“matrix multiplication      |
| `vector_library.py`        | Mini vector class (`add`, `dot`, `norm`, etc.)    |

âœ… All results are **verified against NumPy** for correctness.

---

## ğŸ“Š Applied Work

### ğŸ““ `notebooks/`

**California Housing EDA**

- Dataset loaded and explored
- Manual correlation calculations
- Data interpreted as matrices
- Visualizations using `matplotlib`

---

## ğŸ§ª Key Experiments

- Compared **manual implementations vs NumPy**
- Visualized matrix transformations
- Explained dot product as a **similarity measure**
- Demonstrated how linear algebra enables:
  - Linear regression
  - Gradient descent
  - Feature interactions

---

## ğŸ“š Learning Resources Used

- ğŸ“ Andrew Ng â€” ML Foundations & Linear Algebra Review
- ğŸ“˜ _Hands-On Machine Learning_ â€” Chapters 1â€“2
- ğŸ¥ 3Blue1Brown â€” _Essence of Linear Algebra_

---

## ğŸ§¾ Deliverables Checklist

- [x] Vector operations implemented from scratch
- [x] Dot product implemented & explained intuitively
- [x] Matrix multiplication coded manually
- [x] NumPy verification completed
- [x] California Housing EDA notebook
- [x] Notes & reflections documented

---

## ğŸ¯ Key Takeaways

- Linear algebra is **not abstract math** â€” itâ€™s how ML _thinks_
- Dot product = similarity â†’ core of prediction
- Matrices = transformations â†’ core of learning
- Implementing from scratch removes **black-box thinking**

---

â¡ï¸ **Next:** _Week 02 â€” Statistics & Linear Regression_
